# Chapter 5.1: Parsing Epic TSVs

*Master the technical details of Epic's TSV format to build robust parsers that handle edge cases correctly.*

Epic's choice of Tab-Separated Values (TSV) format seems simple at first glance, but the devil is in the details. Clinical data contains commas, quotes, newlines, and other special characters that can break naive parsers. This chapter provides production-ready code to parse Epic TSVs correctly, handle edge cases, and process files of any size efficiently.

## Why TSV Parsing Isn't Trivial

Epic chose TSV over CSV for good reasons, but their implementation includes specific rules that standard TSV parsers don't handle:

### The Comma Problem
Clinical text is full of commas. Consider this diagnosis:
```
"Fever, chills, and fatigue"
```
In CSV, this would split into three fields. In TSV, it remains one field because tabs rarely appear in clinical text.

### Epic's Escape Sequences
Epic uses backslash escaping for special characters:
- `\t` → Tab character
- `\n` → Newline
- `\r` → Carriage return  
- `\\` → Literal backslash
- Empty field → NULL (not the string "NULL")

<example-query description="View escape sequences in clinical text">
-- Check if we have any plain text with potential special characters
SELECT 
  COUNT(*) as total_notes,
  SUM(CASE WHEN LENGTH(NOTE_TEXT_) > 100 THEN 1 ELSE 0 END) as long_notes
FROM HNO_PLAIN_TEXT
WHERE NOTE_TEXT_ IS NOT NULL;
</example-query>

### Character Encoding
Epic exports use UTF-8 encoding, sometimes with a Byte Order Mark (BOM). Your parser must handle:
- UTF-8 with BOM (`\xEF\xBB\xBF`)
- UTF-8 without BOM
- Clinical symbols (µg, °F, ±)

---

## JavaScript Implementation

Here's a robust TSV parser that handles Epic's format correctly:

### Basic TSV Parser

```javascript
class EpicTSVParser {
  constructor() {
    // Epic's escape sequences
    this.escapeMap = {
      '\\t': '\t',
      '\\n': '\n',
      '\\r': '\r',
      '\\\\': '\\'
    };
  }

  unescapeField(field) {
    // Replace escape sequences in order
    let result = field;
    for (const [escaped, unescaped] of Object.entries(this.escapeMap)) {
      result = result.replace(new RegExp(escaped.replace(/\\/g, '\\\\'), 'g'), unescaped);
    }
    return result;
  }

  parseLine(line) {
    // Split by tabs and unescape each field
    const fields = line.split('\t');
    return fields.map(field => {
      // Empty string means NULL in Epic
      if (field === '') return null;
      return this.unescapeField(field);
    });
  }

  parse(content) {
    // Remove BOM if present
    if (content.charCodeAt(0) === 0xFEFF) {
      content = content.slice(1);
    }

    // Split into lines, handling different line endings
    const lines = content.replace(/\r\n/g, '\n').replace(/\r/g, '\n').split('\n');
    
    // Remove empty trailing line if exists
    if (lines[lines.length - 1] === '') {
      lines.pop();
    }

    if (lines.length === 0) return { headers: [], rows: [] };

    // First line is always headers
    const headers = this.parseLine(lines[0]);
    
    // Parse remaining lines
    const rows = [];
    for (let i = 1; i < lines.length; i++) {
      const fields = this.parseLine(lines[i]);
      
      // Create object with header keys
      const row = {};
      headers.forEach((header, index) => {
        row[header] = fields[index];
      });
      rows.push(row);
    }

    return { headers, rows };
  }
}

// Usage example
const parser = new EpicTSVParser();
const fileContent = fs.readFileSync('pat_enc.tsv', 'utf8');
const { headers, rows } = parser.parse(fileContent);

console.log(`Parsed ${rows.length} encounters`);
console.log('First encounter:', rows[0]);
```

### Stream Parser for Large Files

For files too large to fit in memory, use a streaming approach:

```javascript
const fs = require('fs');
const readline = require('readline');

class EpicTSVStreamParser {
  constructor(filePath, options = {}) {
    this.filePath = filePath;
    this.batchSize = options.batchSize || 1000;
    this.parser = new EpicTSVParser();
  }

  async *parse() {
    const fileStream = fs.createReadStream(this.filePath, { encoding: 'utf8' });
    const rl = readline.createInterface({
      input: fileStream,
      crlfDelay: Infinity // Handle Windows line endings
    });

    let headers = null;
    let batch = [];
    let lineNumber = 0;

    for await (const line of rl) {
      lineNumber++;

      // Skip BOM on first line
      const cleanLine = lineNumber === 1 && line.charCodeAt(0) === 0xFEFF
        ? line.slice(1)
        : line;

      if (!headers) {
        headers = this.parser.parseLine(cleanLine);
        continue;
      }

      const fields = this.parser.parseLine(cleanLine);
      const row = {};
      
      headers.forEach((header, index) => {
        row[header] = fields[index];
      });

      batch.push(row);

      if (batch.length >= this.batchSize) {
        yield batch;
        batch = [];
      }
    }

    // Yield remaining rows
    if (batch.length > 0) {
      yield batch;
    }
  }

  async processFile(callback) {
    let totalRows = 0;
    for await (const batch of this.parse()) {
      await callback(batch);
      totalRows += batch.length;
    }
    return totalRows;
  }
}

// Usage for large files
const streamParser = new EpicTSVStreamParser('hsp_transactions.tsv');

await streamParser.processFile(async (batch) => {
  // Process batch of rows
  for (const row of batch) {
    // Insert into database, transform, etc.
    await processRow(row);
  }
});
```

---

## Python Implementation

Python's flexibility makes it excellent for Epic TSV processing:

### Core Parser Class

```python
import csv
import io
from typing import List, Dict, Iterator, Optional

class EpicTSVParser:
    """Parse Epic TSV files with proper escape handling"""
    
    def __init__(self):
        self.escape_sequences = {
            '\\t': '\t',
            '\\n': '\n',
            '\\r': '\r',
            '\\\\': '\\'
        }
    
    def unescape_field(self, field: str) -> Optional[str]:
        """Unescape Epic field values"""
        # Empty string means NULL
        if field == '':
            return None
            
        # Replace escape sequences
        result = field
        for escaped, unescaped in self.escape_sequences.items():
            result = result.replace(escaped, unescaped)
        return result
    
    def parse_file(self, filepath: str, encoding: str = 'utf-8-sig') -> Iterator[Dict[str, Optional[str]]]:
        """Parse TSV file and yield row dictionaries"""
        with open(filepath, 'r', encoding=encoding) as f:
            # Use csv module with tab delimiter
            reader = csv.DictReader(f, delimiter='\t')
            
            for row in reader:
                # Unescape all fields
                yield {
                    key: self.unescape_field(value)
                    for key, value in row.items()
                }
    
    def parse_to_dataframe(self, filepath: str) -> 'pd.DataFrame':
        """Parse TSV directly to pandas DataFrame"""
        import pandas as pd
        
        # Read with pandas
        df = pd.read_csv(
            filepath,
            sep='\t',
            encoding='utf-8-sig',
            na_values=[''],  # Empty string is NULL
            keep_default_na=True,
            dtype=str  # Read everything as string initially
        )
        
        # Unescape all string columns
        for col in df.columns:
            if df[col].dtype == 'object':
                df[col] = df[col].apply(
                    lambda x: self.unescape_field(x) if pd.notna(x) else None
                )
        
        return df

# Usage example
parser = EpicTSVParser()

# Parse to dictionary rows
for row in parser.parse_file('patient.tsv'):
    print(f"Patient {row['PAT_ID']}: {row['PAT_NAME']}")

# Parse to pandas DataFrame
df = parser.parse_to_dataframe('pat_enc.tsv')
print(f"Loaded {len(df)} encounters")
```

### Optimized Parser for Specific Tables

When you know the schema, optimize with data types:

```python
from datetime import datetime
from decimal import Decimal
from typing import Any, Dict, Type

class TypedEpicParser(EpicTSVParser):
    """Parser with automatic type conversion"""
    
    # Define schemas for common tables
    SCHEMAS = {
        'patient': {
            'PAT_ID': str,
            'PAT_NAME': str,
            'BIRTH_DATE': 'date',
            'ZIP': str
        },
        'pat_enc': {
            'PAT_ID': str,
            'PAT_ENC_CSN_ID': str,
            'CONTACT_DATE': 'datetime',
            'APPT_STATUS_C': int
        },
        'hsp_transactions': {
            'TX_ID': str,
            'TX_AMOUNT': Decimal,
            'TX_POST_DATE': 'date',
            'BILLING_PROV_ID': str
        }
    }
    
    def convert_field(self, value: Optional[str], field_type: Any) -> Any:
        """Convert field to appropriate type"""
        if value is None:
            return None
            
        if field_type == 'date':
            return datetime.strptime(value, '%Y-%m-%d').date()
        elif field_type == 'datetime':
            # Handle multiple datetime formats
            for fmt in ['%Y-%m-%d %H:%M:%S', '%Y-%m-%d']:
                try:
                    return datetime.strptime(value, fmt)
                except ValueError:
                    continue
            raise ValueError(f"Unable to parse datetime: {value}")
        elif field_type == int:
            return int(value) if value else None
        elif field_type == Decimal:
            return Decimal(value) if value else None
        else:
            return value
    
    def parse_typed(self, table_name: str, filepath: str) -> Iterator[Dict[str, Any]]:
        """Parse with automatic type conversion"""
        schema = self.SCHEMAS.get(table_name, {})
        
        for row in self.parse_file(filepath):
            typed_row = {}
            for key, value in row.items():
                field_type = schema.get(key, str)
                try:
                    typed_row[key] = self.convert_field(value, field_type)
                except ValueError as e:
                    # Log error but don't fail entire parse
                    print(f"Warning: {e} in field {key}")
                    typed_row[key] = value
            
            yield typed_row

# Usage with type conversion
typed_parser = TypedEpicParser()

for encounter in typed_parser.parse_typed('pat_enc', 'pat_enc.tsv'):
    # CONTACT_DATE is now a datetime object
    print(f"Encounter on {encounter['CONTACT_DATE'].strftime('%Y-%m-%d')}")
```

---

## Shell Script Utilities

For quick analysis and ETL pipelines, shell scripts are invaluable:

### TSV Processing Functions

```bash
#!/bin/bash

# Epic TSV utility functions

# Count rows (excluding header)
tsv_count() {
    local file=$1
    echo $(($(wc -l < "$file") - 1))
}

# Extract specific columns
tsv_columns() {
    local file=$1
    shift
    local cols="$@"
    
    # Build awk print statement
    local print_stmt=""
    for col in $cols; do
        if [ -z "$print_stmt" ]; then
            print_stmt="\$$col"
        else
            print_stmt="$print_stmt\"\\t\"\$$col"
        fi
    done
    
    awk -F'\t' "{print $print_stmt}" "$file"
}

# Get column numbers for headers
tsv_header_positions() {
    local file=$1
    head -1 "$file" | tr '\t' '\n' | nl -v0
}

# Filter by column value
tsv_filter() {
    local file=$1
    local col=$2
    local value=$3
    
    awk -F'\t' -v col="$col" -v val="$value" \
        'NR==1 || $col==val' "$file"
}

# Convert TSV to SQL INSERT statements
tsv_to_sql() {
    local file=$1
    local table=$2
    
    # Get headers
    IFS=$'\t' read -r -a headers < "$file"
    
    # Generate INSERT statements
    tail -n +2 "$file" | while IFS=$'\t' read -r -a values; do
        echo -n "INSERT INTO $table ("
        echo -n "${headers[*]}" | tr ' ' ','
        echo -n ") VALUES ("
        
        # Quote and escape values
        first=true
        for val in "${values[@]}"; do
            if [ "$first" = true ]; then
                first=false
            else
                echo -n ","
            fi
            
            if [ -z "$val" ]; then
                echo -n "NULL"
            else
                # Escape single quotes
                escaped="${val//\'/\'\'}"
                echo -n "'$escaped'"
            fi
        done
        echo ");"
    done
}

# Split large TSV into chunks
tsv_split() {
    local file=$1
    local lines_per_file=${2:-100000}
    local basename="${file%.tsv}"
    
    # Save header
    head -1 "$file" > "${basename}_header.tsv"
    
    # Split body
    tail -n +2 "$file" | split -l "$lines_per_file" - "${basename}_"
    
    # Add header to each chunk
    for chunk in "${basename}_"*; do
        if [[ ! "$chunk" =~ _header\.tsv$ ]]; then
            cat "${basename}_header.tsv" "$chunk" > "$chunk.tsv"
            rm "$chunk"
        fi
    done
    
    rm "${basename}_header.tsv"
}

# Example usage:
# Count encounters
echo "Total encounters: $(tsv_count pat_enc.tsv)"

# Extract patient ID and encounter date
tsv_columns pat_enc.tsv 1 3 > encounters_summary.tsv

# Filter for specific encounter types
tsv_filter pat_enc.tsv 5 "Office Visit" > office_visits.tsv

# Convert to SQL
tsv_to_sql patient.tsv PATIENT > patient_inserts.sql

# Split large file
tsv_split hsp_transactions.tsv 500000
```

### Validation Script

```bash
#!/bin/bash

# Validate Epic TSV export integrity

validate_epic_export() {
    local export_dir=$1
    local errors=0
    
    echo "Validating Epic EHI Export in: $export_dir"
    echo "========================================="
    
    # Check for manifest
    if [ ! -f "$export_dir/export_manifest.tsv" ]; then
        echo "ERROR: Missing export_manifest.tsv"
        ((errors++))
    fi
    
    # Validate each TSV file
    for tsv in "$export_dir"/*.tsv; do
        if [ -f "$tsv" ]; then
            filename=$(basename "$tsv")
            echo -n "Checking $filename... "
            
            # Check UTF-8 encoding
            if file -b "$tsv" | grep -q "UTF-8"; then
                echo -n "UTF-8 OK, "
            else
                echo -n "ENCODING ERROR, "
                ((errors++))
            fi
            
            # Check for consistent column count
            first_cols=$(head -1 "$tsv" | tr '\t' '\n' | wc -l)
            inconsistent=$(awk -F'\t' -v cols="$first_cols" \
                'NF != cols {print NR}' "$tsv" | head -5)
            
            if [ -z "$inconsistent" ]; then
                echo -n "Columns OK, "
            else
                echo -n "COLUMN COUNT ERROR (lines: $inconsistent), "
                ((errors++))
            fi
            
            # Check for header
            if [ -s "$tsv" ]; then
                echo "Headers present"
            else
                echo "EMPTY FILE"
                ((errors++))
            fi
        fi
    done
    
    echo "========================================="
    if [ $errors -eq 0 ]; then
        echo "Validation PASSED"
        return 0
    else
        echo "Validation FAILED with $errors errors"
        return 1
    fi
}

# Run validation
validate_epic_export /path/to/export/
```

---

## Performance Optimization

### Memory-Efficient Processing

For multi-gigabyte files, memory management is critical:

```python
import mmap
import os

class MemoryEfficientParser:
    """Use memory mapping for huge files"""
    
    def __init__(self, filepath: str):
        self.filepath = filepath
        self.file_size = os.path.getsize(filepath)
    
    def count_rows_fast(self) -> int:
        """Count rows without loading file into memory"""
        with open(self.filepath, 'rb') as f:
            with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mmapped:
                return mmapped.read().count(b'\n')
    
    def process_in_chunks(self, chunk_size: int = 100_000_000):  # 100MB chunks
        """Process file in memory-mapped chunks"""
        with open(self.filepath, 'rb') as f:
            with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mmapped:
                chunk_start = 0
                
                while chunk_start < self.file_size:
                    # Find end of chunk at line boundary
                    chunk_end = min(chunk_start + chunk_size, self.file_size)
                    if chunk_end < self.file_size:
                        # Backtrack to previous newline
                        while chunk_end > chunk_start and mmapped[chunk_end-1:chunk_end] != b'\n':
                            chunk_end -= 1
                    
                    # Process chunk
                    chunk = mmapped[chunk_start:chunk_end].decode('utf-8-sig')
                    yield chunk
                    
                    chunk_start = chunk_end

# Usage
parser = MemoryEfficientParser('huge_table.tsv')
print(f"Total rows: {parser.count_rows_fast()}")

for chunk in parser.process_in_chunks():
    # Process chunk
    process_chunk(chunk)
```

### Parallel Processing

Process multiple files simultaneously:

```python
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing as mp

def process_tsv_file(filepath: str) -> Dict[str, Any]:
    """Process single TSV file"""
    parser = EpicTSVParser()
    row_count = 0
    error_count = 0
    
    try:
        for row in parser.parse_file(filepath):
            row_count += 1
            # Validate row
            if not validate_row(row):
                error_count += 1
    except Exception as e:
        return {
            'file': filepath,
            'status': 'error',
            'error': str(e)
        }
    
    return {
        'file': filepath,
        'status': 'success',
        'rows': row_count,
        'errors': error_count
    }

def parallel_process_export(export_dir: str):
    """Process all TSV files in parallel"""
    tsv_files = [
        os.path.join(export_dir, f)
        for f in os.listdir(export_dir)
        if f.endswith('.tsv')
    ]
    
    # Use number of CPUs minus 1 to leave system responsive
    max_workers = max(1, mp.cpu_count() - 1)
    
    results = []
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        # Submit all files for processing
        future_to_file = {
            executor.submit(process_tsv_file, filepath): filepath
            for filepath in tsv_files
        }
        
        # Collect results as they complete
        for future in as_completed(future_to_file):
            filepath = future_to_file[future]
            try:
                result = future.result()
                results.append(result)
                print(f"Completed: {os.path.basename(filepath)}")
            except Exception as e:
                print(f"Failed: {os.path.basename(filepath)} - {e}")
    
    return results

# Process entire export in parallel
results = parallel_process_export('/path/to/export/')
```

---

## Error Handling and Validation

### Comprehensive Validation

<example-query description="Check for data quality issues in encounters">
-- Look for common Epic data quality issues
SELECT 
  COUNT(*) as total_encounters,
  COUNT(DISTINCT PAT_ID) as unique_patients,
  COUNT(CASE WHEN HSP_ACCOUNT_ID IS NULL THEN 1 END) as missing_har,
  COUNT(CASE WHEN CONTACT_DATE IS NULL THEN 1 END) as missing_date,
  COUNT(CASE WHEN HOSP_ADMSN_TYPE_C_NAME IS NULL THEN 1 END) as missing_admission_type
FROM PAT_ENC;
</example-query>

```python
class EpicDataValidator:
    """Validate Epic TSV data quality"""
    
    def __init__(self):
        self.validation_rules = {
            'patient': {
                'required_fields': ['PAT_ID', 'PAT_NAME'],
                'unique_fields': ['PAT_ID'],
                'date_fields': ['BIRTH_DATE', 'DEATH_DATE']
            },
            'pat_enc': {
                'required_fields': ['PAT_ID', 'PAT_ENC_CSN_ID'],
                'unique_fields': ['PAT_ENC_CSN_ID'],
                'date_fields': ['CONTACT_DATE', 'HOSP_ADMSN_TIME', 'HOSP_DISCH_TIME'],
                'reference_fields': [('PAT_ID', 'patient')]
            }
        }
    
    def validate_file(self, table_name: str, filepath: str) -> Dict[str, Any]:
        """Validate TSV file against rules"""
        rules = self.validation_rules.get(table_name, {})
        parser = EpicTSVParser()
        
        issues = {
            'missing_required': [],
            'duplicate_keys': [],
            'invalid_dates': [],
            'orphaned_references': []
        }
        
        seen_keys = {}
        row_count = 0
        
        for row_num, row in enumerate(parser.parse_file(filepath), start=2):
            row_count += 1
            
            # Check required fields
            for field in rules.get('required_fields', []):
                if not row.get(field):
                    issues['missing_required'].append({
                        'row': row_num,
                        'field': field
                    })
            
            # Check unique fields
            for field in rules.get('unique_fields', []):
                value = row.get(field)
                if value:
                    if value in seen_keys:
                        issues['duplicate_keys'].append({
                            'row': row_num,
                            'field': field,
                            'value': value,
                            'first_seen': seen_keys[value]
                        })
                    else:
                        seen_keys[value] = row_num
            
            # Validate dates
            for field in rules.get('date_fields', []):
                value = row.get(field)
                if value:
                    try:
                        # Try parsing common Epic date formats
                        parsed = None
                        for fmt in ['%Y-%m-%d', '%Y-%m-%d %H:%M:%S']:
                            try:
                                parsed = datetime.strptime(value, fmt)
                                break
                            except ValueError:
                                continue
                        
                        if not parsed:
                            raise ValueError("No format matched")
                            
                        # Check for suspicious dates
                        if parsed.year < 1900 or parsed.year > 2100:
                            issues['invalid_dates'].append({
                                'row': row_num,
                                'field': field,
                                'value': value,
                                'reason': 'Year out of range'
                            })
                    
                    except ValueError:
                        issues['invalid_dates'].append({
                            'row': row_num,
                            'field': field,
                            'value': value,
                            'reason': 'Invalid format'
                        })
        
        return {
            'table': table_name,
            'file': filepath,
            'total_rows': row_count,
            'issues': issues,
            'valid': all(len(v) == 0 for v in issues.values())
        }

# Usage
validator = EpicDataValidator()
result = validator.validate_file('pat_enc', 'pat_enc.tsv')

if not result['valid']:
    print(f"Validation failed for {result['table']}:")
    for issue_type, issues in result['issues'].items():
        if issues:
            print(f"  {issue_type}: {len(issues)} issues")
```

### Common Epic-Specific Issues

Handle these frequent problems:

```python
def handle_epic_quirks(row: Dict[str, Any]) -> Dict[str, Any]:
    """Fix common Epic data quirks"""
    
    # 1. Default dates (1900-01-01 means "not set")
    for key, value in row.items():
        if isinstance(value, str) and value == '1900-01-01':
            row[key] = None
    
    # 2. Placeholder IDs (often negative or very high numbers)
    if 'PAT_ID' in row and row['PAT_ID']:
        if row['PAT_ID'].startswith('-') or row['PAT_ID'] == '999999999':
            row['_is_placeholder'] = True
    
    # 3. Status codes that are actually NULL
    status_fields = [k for k in row.keys() if k.endswith('_C')]
    for field in status_fields:
        if row[field] == '0':  # Often means "not set"
            row[field] = None
    
    # 4. Trim all string fields
    for key, value in row.items():
        if isinstance(value, str):
            row[key] = value.strip()
    
    return row
```

---

## Key Takeaways

1. **Epic TSVs use backslash escaping** - Always unescape `\t`, `\n`, `\r`, and `\\`
2. **Empty string means NULL** - Not the string "NULL" or "\N"
3. **Headers are always present** - First row contains column names
4. **UTF-8 encoding is standard** - But may include BOM
5. **Large files need streaming** - Don't load gigabytes into memory
6. **Validation is critical** - Epic data has quirks that need handling
7. **Performance matters** - Use parallel processing for multi-file exports

Remember: Epic chose TSV for good reasons. Commas appear everywhere in clinical data, but tabs are rare. Build your parsers to handle the edge cases, and you'll have a robust foundation for all your Epic data processing needs.

<example-query description="Verify TSV parsing concepts with real data">
-- Check for various data patterns in our sample
SELECT 
  'Encounters' as table_name,
  COUNT(*) as total_rows,
  COUNT(DISTINCT PAT_ID) as unique_patients,
  COUNT(CASE WHEN HOSP_ADMSN_TYPE_C_NAME LIKE '%,%' THEN 1 END) as fields_with_commas,
  MIN(CONTACT_DATE) as earliest_date,
  MAX(CONTACT_DATE) as latest_date
FROM PAT_ENC
UNION ALL
SELECT 
  'Providers' as table_name,
  COUNT(*) as total_rows,
  COUNT(DISTINCT PROV_ID) as unique_providers,
  COUNT(CASE WHEN PROV_NAME LIKE '%,%' THEN 1 END) as names_with_commas,
  NULL as earliest_date,
  NULL as latest_date
FROM CLARITY_SER;
</example-query>

---

## Next Steps
→ Continue to Chapter 5.2: Data Quality & Validation Recipes