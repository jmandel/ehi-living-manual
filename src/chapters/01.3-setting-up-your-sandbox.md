# Chapter 1.3: Setting Up Your Sandbox

*Purpose: Transform your Epic EHI export from a folder of TSV files into a queryable, performant database environment where you can safely explore and learn.*

### The Challenge: From TSV Files to Working Database

You're staring at a folder with hundreds of TSV files—some tiny, some gigabytes in size. Your mission is to transform this into a working database where you can run queries, explore relationships, and build understanding. This chapter provides battle-tested approaches for creating your Epic data sandbox.

### Why SQLite? The Pragmatic Choice

**SQLite** is the perfect learning environment for Epic EHI data:
- **Zero Configuration**: No server setup, no admin passwords
- **Single File**: Your entire database in one portable file
- **Industry Standard**: Used by billions of devices worldwide
- **Excellent Performance**: Handles multi-GB databases with ease
- **SQL Compatible**: Skills transfer to any database platform

### The Basic Import Script

Here's a straightforward script that converts your TSV files into a SQLite database:

```bash
#!/bin/bash
# make-db.sh - Convert Epic TSV files to SQLite database

DB_NAME="epic.db"
TSV_DIR="${1:-.}"  # Default to current directory

# Remove existing database
rm -f "$DB_NAME"

# Import each TSV file
for tsv in "$TSV_DIR"/*.tsv; do
    if [ -f "$tsv" ]; then
        table=$(basename "$tsv" .tsv)
        echo "Importing $table..."
        
        sqlite3 "$DB_NAME" <<EOF
.mode tabs
.import "$tsv" "$table"
EOF
    fi
done

echo "Database created: $DB_NAME"
echo "Tables imported: $(sqlite3 "$DB_NAME" "SELECT COUNT(*) FROM sqlite_master WHERE type='table';")"
```

### Production-Ready Import Process

For real-world Epic exports, you need more sophistication. This enhanced script handles large files efficiently:

```bash
#!/bin/bash
# make-db-enhanced.sh - Production-ready Epic EHI import

DB_NAME="epic.db"
LOG_FILE="import.log"
TSV_DIR="${1:-.}"

# Initialize database with performance settings
echo "[$(date)] Starting import process" > "$LOG_FILE"

sqlite3 "$DB_NAME" <<EOF
-- Disable safety features during import
PRAGMA journal_mode = OFF;
PRAGMA synchronous = OFF;
PRAGMA cache_size = -2000000;  -- 2GB cache
PRAGMA temp_store = MEMORY;
PRAGMA mmap_size = 10737418240; -- 10GB memory map
EOF

# Import with progress tracking
total_files=$(ls -1 "$TSV_DIR"/*.tsv 2>/dev/null | wc -l)
current=0

for tsv in "$TSV_DIR"/*.tsv; do
    if [ -f "$tsv" ]; then
        table=$(basename "$tsv" .tsv)
        rows=$(wc -l < "$tsv")
        current=$((current + 1))
        
        echo "[$(date)] [$current/$total_files] Importing $table ($rows rows)..." | tee -a "$LOG_FILE"
        
        # Time the import
        start_time=$(date +%s)
        
        sqlite3 "$DB_NAME" <<EOF 2>> "$LOG_FILE"
.mode tabs
.import "$tsv" "$table"
EOF
        
        end_time=$(date +%s)
        duration=$((end_time - start_time))
        echo "  Completed in ${duration}s" | tee -a "$LOG_FILE"
    fi
done

# Re-enable safety features and optimize
echo "[$(date)] Optimizing database..." | tee -a "$LOG_FILE"
sqlite3 "$DB_NAME" <<EOF
PRAGMA journal_mode = WAL;
PRAGMA synchronous = NORMAL;
VACUUM;
ANALYZE;
EOF

echo "[$(date)] Import complete!" | tee -a "$LOG_FILE"
```

### Essential Indexes for Performance

Epic tables often lack indexes in the export. Add these critical indexes after import:

```sql
-- create-indexes.sql
-- Run after import: sqlite3 epic.db < create-indexes.sql

-- Patient and encounter indexes (most critical)
CREATE INDEX idx_patient_id ON PATIENT(PAT_ID);
CREATE INDEX idx_pat_enc_pat ON PAT_ENC(PAT_ID);
CREATE INDEX idx_pat_enc_csn ON PAT_ENC(PAT_ENC_CSN_ID);
CREATE INDEX idx_pat_enc_date ON PAT_ENC(CONTACT_DATE);

-- Financial indexes (if using billing data)
CREATE INDEX idx_hsp_acct_id ON HSP_ACCOUNT(HSP_ACCOUNT_ID);
CREATE INDEX idx_hsp_acct_pat ON HSP_ACCOUNT(PAT_ID);
CREATE INDEX idx_arpb_tx_pat ON ARPB_TRANSACTIONS(PATIENT_ID);

-- Provider and department indexes
CREATE INDEX idx_clarity_ser_prov ON CLARITY_SER(PROV_ID);
CREATE INDEX idx_clarity_dep_id ON CLARITY_DEP(DEPARTMENT_ID);

-- Diagnosis indexes
CREATE INDEX idx_dx_list_acct ON HSP_ACCT_DX_LIST(HSP_ACCOUNT_ID);

-- Let SQLite optimize query plans
ANALYZE;
```

### VS Code: Your SQL Development Environment

**VS Code** with the right extensions becomes a powerful Epic data exploration tool.

#### Installation Steps:
1. Install VS Code
2. Install these extensions:
   - **SQLTools** (by Matheus Teixeira)
   - **SQLTools SQLite** (by Matheus Teixeira)

#### Configuration:
Create `.vscode/settings.json` in your project:

```json
{
  "sqltools.connections": [
    {
      "name": "Epic EHI",
      "driver": "SQLite",
      "database": "${workspaceFolder}/epic.db",
      "connectionTimeout": 30
    }
  ],
  "sqltools.format": {
    "language": "sql",
    "reservedWordCase": "upper"
  }
}
```

#### Features You Get:
- **Schema Browser**: Navigate tables and columns visually
- **Autocomplete**: Table and column names as you type
- **Query History**: Never lose a useful query
- **Export Results**: Save query results as CSV/JSON
- **Execution Plans**: Understand query performance

### Command-Line Power User Setup

For those who prefer the terminal:

```bash
# Create useful aliases
echo "alias epic='sqlite3 -column -header epic.db'" >> ~/.bashrc
echo "alias epic-tables='sqlite3 epic.db \".tables\"'" >> ~/.bashrc
echo "alias epic-schema='sqlite3 epic.db \".schema\"'" >> ~/.bashrc

# Source the changes
source ~/.bashrc

# Now you can:
epic "SELECT COUNT(*) FROM PAT_ENC"
epic-tables | grep HSP
epic-schema PATIENT
```

### Python Analysis Environment

Set up a Python environment for data analysis:

```python
# epic_explorer.py - Reusable Epic data exploration class
import sqlite3
import pandas as pd
from pathlib import Path

class EpicExplorer:
    def __init__(self, db_path='epic.db'):
        """Initialize connection to Epic SQLite database."""
        self.db_path = Path(db_path)
        if not self.db_path.exists():
            raise FileNotFoundError(f"Database not found: {db_path}")
        self.conn = sqlite3.connect(str(self.db_path))
        
    def list_tables(self, pattern=None):
        """List all tables, optionally filtered by pattern."""
        query = """
        SELECT name 
        FROM sqlite_master 
        WHERE type='table'
        """
        if pattern:
            query += f" AND name LIKE '%{pattern}%'"
        query += " ORDER BY name"
        return pd.read_sql(query, self.conn)
    
    def table_info(self, table_name):
        """Get detailed column information for a table."""
        # First, try to get schema with comments
        schema_query = f"""
        SELECT sql 
        FROM sqlite_master 
        WHERE type='table' AND name='{table_name}'
        """
        schema = self.conn.execute(schema_query).fetchone()
        if schema:
            print(f"Schema for {table_name}:")
            print(schema[0])
            print("\n")
        
        # Then get column details
        return pd.read_sql(f"PRAGMA table_info({table_name})", self.conn)
    
    def sample_data(self, table_name, n=10):
        """Preview sample rows from a table."""
        return pd.read_sql(f"SELECT * FROM {table_name} LIMIT {n}", self.conn)
    
    def table_stats(self):
        """Get row counts for all tables."""
        tables = self.list_tables()
        stats = []
        
        for table in tables['name']:
            try:
                count = self.conn.execute(
                    f"SELECT COUNT(*) FROM {table}"
                ).fetchone()[0]
                stats.append({
                    'table': table,
                    'row_count': count
                })
            except:
                stats.append({
                    'table': table,
                    'row_count': -1  # Error indicator
                })
        
        df = pd.DataFrame(stats)
        return df.sort_values('row_count', ascending=False)
    
    def find_patient_data(self, patient_id):
        """Find all tables containing data for a specific patient."""
        tables_with_patient = []
        tables = self.list_tables()
        
        for table in tables['name']:
            # Common patient ID column names in Epic
            for col in ['PAT_ID', 'PATIENT_ID', 'PAT_MRN_ID']:
                try:
                    query = f"""
                    SELECT COUNT(*) 
                    FROM {table} 
                    WHERE {col} = '{patient_id}'
                    """
                    count = self.conn.execute(query).fetchone()[0]
                    if count > 0:
                        tables_with_patient.append({
                            'table': table,
                            'column': col,
                            'record_count': count
                        })
                        break
                except:
                    pass
        
        return pd.DataFrame(tables_with_patient)

# Example usage
if __name__ == "__main__":
    explorer = EpicExplorer()
    
    # Show largest tables
    print("Top 10 largest tables:")
    print(explorer.table_stats().head(10))
    print("\n")
    
    # Explore patient encounter table
    print("PAT_ENC sample data:")
    print(explorer.sample_data('PAT_ENC', 5))
```

### Handling Common Import Issues

#### Issue 1: Memory Errors on Large Files

For files over 1GB, use streaming import:

```python
# stream_import.py - Handle huge TSV files
import sqlite3
import csv

def stream_import_tsv(tsv_path, db_path, table_name, chunk_size=10000):
    """Import large TSV file in chunks to avoid memory issues."""
    conn = sqlite3.connect(db_path)
    
    with open(tsv_path, 'r', encoding='utf-8-sig') as f:
        # Read header
        header = f.readline().strip().split('\t')
        
        # Create table from first chunk
        reader = csv.DictReader(f, fieldnames=header, delimiter='\t')
        
        chunks_processed = 0
        for chunk in iter(lambda: list(itertools.islice(reader, chunk_size)), []):
            if chunks_processed == 0:
                # Create table based on first chunk
                df = pd.DataFrame(chunk)
                df.to_sql(table_name, conn, if_exists='replace', index=False)
            else:
                # Append subsequent chunks
                df = pd.DataFrame(chunk)
                df.to_sql(table_name, conn, if_exists='append', index=False)
            
            chunks_processed += 1
            print(f"Processed {chunks_processed * chunk_size:,} rows...")
    
    conn.close()
    print(f"Import complete: {table_name}")
```

#### Issue 2: Special Characters in Data

Epic data can contain tabs, newlines, and other special characters. Handle them properly:

```python
def clean_epic_value(value):
    """Clean special characters in Epic data."""
    if value is None:
        return None
    
    # Epic escape sequences
    replacements = {
        '\\t': '\t',
        '\\n': '\n',
        '\\r': '\r',
        '\\\\': '\\',
    }
    
    for escaped, unescaped in replacements.items():
        value = value.replace(escaped, unescaped)
    
    return value
```

### Query Performance Tips

<example-query description="Check if your indexes are being used">
EXPLAIN QUERY PLAN
SELECT 
  p.PAT_NAME,
  COUNT(e.PAT_ENC_CSN_ID) as visit_count
FROM PATIENT p
JOIN PAT_ENC e ON p.PAT_ID = e.PAT_ID
WHERE e.CONTACT_DATE >= '2018-01-01'
GROUP BY p.PAT_ID, p.PAT_NAME
ORDER BY visit_count DESC
LIMIT 10;
</example-query>

Look for "USING INDEX" in the output. If you see "SCAN TABLE", consider adding indexes.

### Docker Container Option

For team environments, containerize your setup:

```dockerfile
# Dockerfile
FROM python:3.11-slim

# Install SQLite and tools
RUN apt-get update && apt-get install -y \
    sqlite3 \
    git \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Install Python packages
RUN pip install \
    pandas \
    sqlite3 \
    jupyterlab \
    ipython

# Create working directory
WORKDIR /epic-sandbox

# Copy scripts
COPY scripts/ ./scripts/

# Expose Jupyter port
EXPOSE 8888

CMD ["jupyter", "lab", "--ip=0.0.0.0", "--no-browser", "--allow-root"]
```

### Safety and Best Practices

1. **Never Modify Original TSV Files**: Always work with copies
2. **Regular Backups**: Before major operations, backup your database:
   ```bash
   sqlite3 epic.db ".backup epic_backup_$(date +%Y%m%d).db"
   ```
3. **Version Control Your Scripts**: Track all transformation code
4. **Document Everything**: Note any data cleaning or modifications
5. **Test on Samples First**: Before processing 10GB files, test with smaller ones

### Validation Checklist

After setting up your sandbox, verify it's working correctly:

<example-query description="Count total patients in the database">
SELECT COUNT(DISTINCT PAT_ID) as patient_count
FROM PATIENT;
</example-query>

<example-query description="Check encounter date ranges">
SELECT 
  MIN(CONTACT_DATE) as earliest_encounter,
  MAX(CONTACT_DATE) as latest_encounter,
  COUNT(*) as total_encounters
FROM PAT_ENC;
</example-query>

<example-query description="Verify key relationships work">
SELECT 
  'Patients with encounters' as check_type,
  COUNT(DISTINCT p.PAT_ID) as count
FROM PATIENT p
INNER JOIN PAT_ENC e ON p.PAT_ID = e.PAT_ID

UNION ALL

SELECT 
  'Total patients' as check_type,
  COUNT(DISTINCT PAT_ID) as count
FROM PATIENT;
</example-query>

---

### Key Takeaways

1. **SQLite is ideal** for learning Epic data—no server needed, great performance
2. **Use performance pragmas** during import, then re-enable safety features
3. **Indexes are critical**—Epic exports don't include them
4. **VS Code + SQLTools** provides an excellent exploration environment
5. **Python + pandas** enables advanced analysis when SQL isn't enough
6. **Always validate** your import with test queries
7. **Handle large files** with streaming techniques, not brute force

You now have a fully functional sandbox where you can safely explore Epic data. The patterns you learn here will apply whether you're working with one patient or one million.